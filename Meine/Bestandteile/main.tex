\chapter{Introduction}

There are many tools on the internet for building graphs to visualize data. Famous examples are "ConceptDraw Pro", "Lucidchart" \cite{JeffParker} or "draw.io". However when using these the user spends a lot of time on creating a nice looking diagram, centering important components etc., to get a pleasant looking result in the end or they are bound to displaying hardware components.

The idea for this project is to offer an online editor with CRUD-functionality for a network, consisting of components of software projects, as well as a first implementation of an algorithm that will, in most cases, create a nice looking layout on its own.

\section{Purpose}
The purpose is to try out and test a combination of technologies. This software, or variations of it, might later be incorporated into a bigger project or adapted to fit another use case. The experiences and impressions during development can be of help when thinking about what technology stack to use, which is why a big part of this paper is dedicated to documenting this process.

\chapter{Backend to Frontend}

In this chapter we will introduce the individual components, tools and frameworks this application is built with. For some of them we will give some more insights on how they work internally, the others are big enough on their own. The order will be the same as they appeared in the development process.

\section{The GRAND-stack}
GRAND stands in this case for \textbf{G}raphQL, \textbf{R}eact, \textbf{A}pollo and \textbf{N}eo4j \textbf{D}atabase. \cite{GrandStackGettingStarted} React is a frontend framework, Apollo is used for statemanagement on the client side and communicating to the database on the server side. GraphQL will be used for fetching and mutating data. Neo4j is a graph database and the server will communicate with it through a JavaScript driver provided by the Neo4j community. More details can be found in the Development and Documentation section.

\section{Query Language - GraphQL}
GraphQL is a data query language as well as specification. Its development was started by Facebook in 2012 and it was open sourced in 2015. \cite{GraphQLFoundation}

After their application suffered from poor performance on mobile devices, they took a new implementation using natively implemented models and views. This required a new API for their news feed as it was previously delivered as pure HTML.\\
After evaluating different common options like RESTful-APIs and FQL they often saw the same problems:  The ratio of data actually used compared to the one fetched was very small, the number of requests \cite{GraphQLIntro} and the amount of code on both server and client side to prepare the data was big. \cite{EngineeringFB}\\
For example, for loading the start page of a single user, there would have been a lot of different requests necessary:
\begin{itemize}
\item \emph{https://facebook.com/user/id} - Get all user specific data
\item \emph{https://facebook.com/user/id/events} - Get all possibly relevant events
\item \emph{https://facebook.com/user/id/friends-suggestions} - Get all friend suggestions
\item ...
\end{itemize} \cite{GraphQLIntro}

GraphQL aims to resolve all these issues: Reduce the amount of unnecessary data transferred, reduce the number of requests and increase the developer productivity by making it easier to use fetched data. \cite{EngineeringFB}

It allows developers to get a lot of different data from a single endpoint. This means that instead the above shown 3+ endpoints, when using GraphQL all requests would go to \emph{graph.facebook.com}, with a query similar to:\\ (todo: define graphql styles)
\newpage
\begin{lstlisting}[caption={A GraphQL Query},label={ex211}]
query {
	user(id: 1) {
		name
		events {
			count
		}
		friends_suggestions {
			name
			mutual_friends {
				count
			}
		}
	}
}
\end{lstlisting}
\citep[with adaptions]{GraphQLIntro}	
Where the answer would be a JSON-string: (todo: define JSON style)
\begin{lstlisting}[caption={Example Response Data},label={ex212}]
{ 
	"data": {
		"user": {
			"name": "Brandon Minnick",
			"events": {
				"count": 4
			},
			"friends_suggestions": {
				"name": "Seth Juarez",
				"mutual_friends": {
					"count": 18 
				}
			}
		}
	}
}
\end{lstlisting}
\citep[with adaptions]{GraphQLIntro}	

The query can be as extensive as the developer needs it, it will return only the data requested and the answer string can be directly accessed like a JSON-object. By that GraphQL fulfills all its design goals.
\\ \\
The previously shown query then needs to be resolved by a server thats able to interpret GraphQL and resolve the query. All non primitive data types have to be defined following the GraphQL specification. An example for a user schema might be:

\begin{lstlisting}
type User {
	id: ID! 
	name: String! 
	events: [Event] 
	friends: [Friend] 
	friends_suggestions: [Friend_Suggestion] 
}
\end{lstlisting}

\noindent
where "Event", "Friend" and "Friend\_Suggestion" themselves are other types described in a similar manner.

By putting a "!" behind a property the programmer marks it as required, meaning it can never be null or empty. The square brackets define that the property is a list of the type they surround.
\\
To be able to run queries in the first place, we must first define a root type for all queries:

\begin{lstlisting}
schema {
	query: Query
}
\end{lstlisting}

In this root type all possible queries must be described:

\begin{lstlisting}
type Query { 
	user(id: ID!): User 
}
\end{lstlisting}

Here we describe a query that can be executed as shown in
\textbf{\ref{ex211}} by providing an ID to the query and the correct query name, together with a collection set telling the server which fields to fetch. In the parentheses any query arguments are listed, in this example id must be provided. After the double dot the return type is named. 
\\
The server will then make requests to the DB, fetch the requested data and return it to the user once all fields were filled with values.
\\
For further information about the extensive type system please see the official GraphQL specification. \cite{GraphQLSpec}

\section{Database - Neo4j}
\subsubsection{General}
Neo4j is a so called graph database. The idea of graph databases is, compared to traditional relational databases, a young concept and differs in a few concepts. At the moment Neo4j is the 22nd most popular database overall \cite{DbEnginesGeneral} and the most popular graph database. \cite{DbEnginesGraph}

\begin{itemize}
\item Unlike most relational databases, who store data through tables and joins, Neo4j stores data in the form of actual nodes and relationships between such \cite{Neo4jDevGuides}. In other DBMS relations between items generally are achieved through join-/lookup-tables which have to be generated. \cite{RelVsGraph}

\item When running a query on a relation DB the server will run through a table and when it finds the searched item it might look for the ID of a related item and start indexing again. With a graph DB the server will index \citep[minute 32]{NeoInternals} once to find the initial node and can then directly access all connected items as they are stored through their relation with the current one. \cite{WhatGraphDB} These are stored as memory pointers which makes following them extremely efficient.
\item Neo4j uses Cypher as query language. The Cypher syntax was supposed to visually represent the shape of the data a user wants to retrieve instead of describing how to get data, as well as offer the power and functionality other languages offer. \cite{Neo4jCypher} 
\end{itemize}

\subsubsection{Cypher}
For matching all nodes connected to node A through a "Neighbor" relationship, we simply state
\begin{exmp}
\label{ex231}
\emph{MATCH (n:Node \{label: "A"\})-[:Neighbor]-(n2:Node) RETURN n, n2 }
\end{exmp}
Parentheses represent a node, square brackets a relationship. The naming works after the following pattern: <name>:<type>. In this example n is the name for the first node and n2 the name of the list of connected nodes. We didn't specify a name for the relationship as we did not want to retrieve data from it. By using curly braces we can specify certain properties a node or relationship should have. The other way of doing so would be 
\begin{exmp}
\label{ex232}
\emph{MATCH (n:Node)-[:Neighbor]-(n2:Node) WHERE n.label = "A" RETURN n, n2 }
\end{exmp}
which might look a bit cleaner. We could also return only specific values of n and n2 and give them names by stating \\
\emph{ ...RETURN n.label AS Label1, n2.label AS Label2 }\\
The following information about Neo4j internals is all from \cite{NeoInternals} and \cite{Neo4jInternalsPP}. Sadly, these sources are all old and probably outdated, yet there does not seem to be more updated information on the internet.
\subsubsection{The Graph on Disk}
Internally, there are 3 types of records saved on the disk: node-, relationship- and property-records. All of these have fixed sizes to allow for quicker allocation during the start up process. Every record has an "inUse" field, as well as a unique ID with which Neo4j is able to exactly locate a searched record on the disk. \citep[minute 08]{NeoInternals} \\
Properties on nodes are saved through a linked list like object. The exact implementation however does not alter the idea behind it. A property knows about its type and has a next pointer. Each node saves the pointer to its first property whose next pointer will lead to the next property etc.. Should a next pointer be empty the algorithm knows that it has reached all properties of a node. \\
In addition to the first property, each node knows about its first relationship. If a it is the \emph{first} one, is simply being determined by the order of creation. A relationship has pointers to its start- and end-node, to its type and four others to other relationships, which are best explained in an example traversal in pseudo code:
\begin{lstlisting}
if node n has relationship pointer r: 
	if n is start node of r: 
		if r has StartNext pointer sn: 
			set r = sn 
			repeat from line 2 
		endif 
		else  
			visited all relationships $ \rightarrow $ terminate
		endelse 
	endif 
	else 
		if r has EndNext pointer en: 
			set r = en 
			repeat from line 2 
		endif 
		else
			visited all relationships $ \rightarrow $ terminate
		endelse 
	endelse 
endif
\end{lstlisting}

\noindent
We see that every relationship has two next pointers. Which one will be used for further traversal, depends on if the source node is start- or end-node in the current relationship. \\
In addition to this, the same pointers exist into the other direction, meaning that there are also two pointers called StartPrevious and EndPrevious. The question for selecting which one will be chosen for further iteration stays the same.

\subsubsection{The Graph in Memory}
Upon start up these records are being loaded into the "FS Cache" (File System Cache). Neo4j will then partition these into equally sized regions and create a hit counter for each of them, to encounter high traffic regions that will be loaded into the "Node/Relationship Object Cache" which is more similar to an actual graph. \\
Here each node holds a list of relationships that are grouped by the relationship type to allow for quick traversals, and relationships only hold their properties as well as start- and end-node and their type. Any references to other records are being done by its ID. 

\subsubsection{Traversing}
For finding a node to start traversing the graph, Neo4j uses traditional indexing. \citep[minute 32]{NeoInternals} Once the start node is found, 2 concepts take over:
\begin{enumerate}
\item \textbf{RelationshipExpanders} which will for a node return all relevant relationships to continue traversing from that node
\item \textbf{Evaluators} which return if traversing should continue on this branch ($ \rightarrow $ expand) or not and if this node should be included in the result set or not.
\end{enumerate}
When accessing a node the first thing the system will try to do is fetch it from the cache. If it shouldn't be there, the next place that will be checked is the FS Cache. Should the region that contains the node be apparent here, the access is quick but blocking, meaning that the entire region is getting locked. In the case that the region is out of the FS cache the operation is blocking and slower. \\
The locking is necessary to make sure that no other transaction will evict that area from the memory while the current one reads the data.

\subsubsection{Adding Cypher}
As Cypher describes the shape of the searched data, a searched query will be converted into a representative pattern graph that matches the searched structure. \\
When a query is run, the first thing that happens is that matching start-nodes are searched in the database (through indexing). When a node is found, traversing the database starts as described above. For Expanders and Evaluators to know what to return, they simply compare the pattern graph described through Cypher with the graph that was found so far and see if there is more data that matches. 

\section{Server - ApolloServer}
%maybe some general information about Apollo?
Apollo Server is a spec-compliant GraphQL server. It can be embedded into Node.js middleware like Express or Fastify \cite{ApolloServerIntro} and will listen for connections on a defined port. \\
When it receives one it will read the query and call the respective route, or resolvers as they are called. \\
In addition to that the server will deliver, together with some more, a \emph{context} object to each route that contains a driver which connects to a database, which is Neo4j in our case. Using this object together with a specified Cypher query we can manipulate the DB. 

\subsubsection{Example Resolver}
A resolver to create a node might look like the following:

\begin{lstlisting}[caption={A Basic Resolver},label={ex241}]
async CreateNode( _, args, ctx ) { 
	const session = ctx.driver.session(); 
	const query = ` 
			CREATE (n:Node:${ args.nodeType } {id:$id, label:$label, nodeType: $nodeType}) 
			SET n += $props 
			RETURN n`; 
	const results = await session.run(query, args);
	return results.records.map(record => record.get('n').properties)[0]; 
}
\end{lstlisting}

Lets break down whats happening in this piece of code:
\begin{itemize}
\item Line 1 contains the function definition. "args" is an object that contains all data sent with the query from the frontend. "ctx" is the context object that contains the neo4j driver to communicate with the DB. The first argument "\_", which is a placeholder here as we do not need it, is the so called "parent" which is equal to the previous resolver in the resolver chain. (More about this later REMOVE THIS COMMENT)
\item In line 2 we acquire a session to communicate with the database. \cite{Neo4jDriver} Over this object we can send parameters that get executed right away.
\item Lines 3 to 6 define a Cypher query which is similar to the ones shown in \exref{ex231} and \exref{ex232}. 
\item In line 4 we make use of the args object and embed the nodeType directly into the query string by using template strings. This is necessary because at this position of a cypher query we can't make use of query variables the same way we do in the rest of the query. Later in that line we can see that by using \$<variableName> we can access query variables we pass along.
\item Line 5 demonstrates the usage of an object we can pass as query variable. This object can't only contain simple datatypes, but its really useful to set various values at once.
\item Finally, in line 7 we send the specified query string together with the args object (that must contain all referenced variables) to the database. By using the ES6 await keyword we make sure that code execution doesn't continue until the results are returned.
\item In the last line we iterate over the record set and retrieve any properties by the in the query specified name. Using only the first element of the array is specific to this case, as CreateNode is defined to return a single node, not an array of such.
\end{itemize}

\subsubsection{Resolver Chain}
To explain the resolver chain we will take a look at the following example GraphQL query: 
%(https://www.apollographql.com/docs/apollo-server/data/resolvers/#resolver-chains) (with adaptions)
\begin{lstlisting}[label={ex242}]
query GetBooksByLibrary {
	libraries { 
		branch 
		books { 
			title 
			author { 
				name 
			} 
		} 
	} 
}
\end{lstlisting}

which will be executed on this schema 
%(https://www.apollographql.com/docs/apollo-server/data/resolvers/#resolver-chains) (with adaptions)
\begin{lstlisting}[caption={Schema Definition},label={ex243}]
# A library has a branch and books 
type Library { 
	branch: String! 
	books: [Book!] 
} 

# A book has a title and author 
type Book { 
	title: String! 
	author: Author! 
	branch: String! 
} 

# An author has a name 
type Author { 
	name: String!
} 

type Query { 
	libraries: [Library] 
}
\end{lstlisting}

\noindent
To resolve the query we need 4 resolvers:
\begin{itemize}
\item A root resolver which defines the entry point for the query
\item One resolver each for "Library", "Book" and "Author"
\end{itemize}

\noindent
Assuming we have static arrays called "libraries", "books" and "authors" that are filled with data, the resolvers might look like the following: 
%(https://www.apollographql.com/docs/apollo-server/data/resolvers/#resolver-chains) (with adaptions)
\begin{lstlisting}[caption={Resolver Definition},label={ex244}]
const resolvers = { 
	Query: { 
		libraries() { 
			return libraries; 
		} 
	}, 
	Library: { 
		branch(parent) { 
			return parent.branch; 
		}, 
		books(parent) { 
			return books.filter(book => book.branch === parent.branch); 
		} 
	}, 
	Book: { 
		title(parent) { 
			return parent.title; 
		}, 
		author(parent) { 
			return authors.find(author => author.name === parent.author.name); 
		} 
	}, 
	Author: { 
		name(parent) { 
			return parent.name; 
		} 
	}
};
\end{lstlisting}

First, the Query resolver is hit and it will search for a defined key that is similar to the name mentioned in the highest level of the query object in \textbf{\ref{ex241}}, in this case "libraries". In the GraphQL schema under \textbf{\ref{ex243}} we defined that this query will return an array of Library objects. \\
Knowing this, the server will now go through each object of this array and look for resolvers of the in the query specified fields. This object is passed as \emph{parent} into the next resolver in the resolver chain. \\
For each library we want the branch and an array of books. As branch is a primitive type it does not need to be further resolved. Books however, returns an array of non-primitive types. To find out which books we need to return we can access the value \emph{parent.branch} and compare it to the branch of each book in the books array and return those who match. \\
books is again an array of a non primitive type and has to be further resolved by iterating through the array and accessing the requested values title and author. Title is just a string, whereas author will get resolved further etc.

\subsubsection{Cypher in GraphQL}
Using GraphQL directives we can "annotate" our schema and specify precisely certain actions or checks the server should perform when accessing a field. \\
We could create the following schema: 
%(https://www.graphql-tools.com/docs/schema-directives/#implementing-schema-directives)

\begin{lstlisting}[caption={Example Directive Declaration}]
directive @deprecated( 
	reason: String = "No longer supported" 
) on FIELD_DEFINITION | ENUM_VALUE 

type ExampleType { 
	newField: String 
	oldField: String @deprecated(reason: "Use `newField`.")
}
\end{lstlisting}

Directives can be distinguished by the @-symbol and are placed after a field definition to annotate one. When querying \emph{oldField} on \emph{ExampleType} the server might only respond with "Use 'newField'" and not send any data. The exact behavior depends on how directive behavior is defined in the server. \\
The use cases range from formatting strings, enforcing access permissions to value checking when the client sends data and many more. For information about how directives can be implemented please refer to
%(https://www.graphql-tools.com/docs/schema-directives/#implementing-schema-directives)
\\
In the GRAND-stack we can use a pre-defined directive called "@cypher" and through that use cypher statements directly in the schema definition file. A great and short example is getting all connected nodes for a specific node:

\begin{lstlisting}[caption={Cypher in GraphQL}]
type Node { 
	... 
	connectedTo: [Node] @cypher(statement: "MATCH (this)--(:Link)--(n:Node) return n") 
	...
}
\end{lstlisting}

The node that is currently being iterated over in the resolver chain is passed as \emph{this} to Neo4j. Then it'll look for other nodes that are connected through any relationship of type \emph{Link} and return these. In addition to this, ApolloServer can generate default resolvers for queries and mutations meaning we do not have to write a resolver for \emph{Node} on our own. This combination makes writing query resolvers a rare occasion when using the GRAND-stack. \\

Please note that this feature is only available when \emph{APOC} is enabled on this Neo4j instance (more information about this in the end of chapter 3 and chapter 4).

\section{Frontend - React}
React is a JavaScript-Framework to create component-based user interfaces. Each component has to define a \emph{render} method which describes what appears on the screen. In this method the programmer writes basic HTML or can embed other react components. \\
The standard \emph{index.html} is pretty short when using react. The only code a programmer writes there is normally in the header area to include CDNs or other resources. The body contains only one element: \\
\emph{<div> id="root"></div>} 

\subsubsection{Components}
In \emph{index.js} this root div will be referenced by the react-internal render method and recursively build the basic html out of the defined react components:

\begin{lstlisting}[caption={Hello World in React},label={ex251}]
// index.html:
<!DOCTYPE html>
<html>
<head>
	<title>Intro-App</title>
</head>
<body>
	<div id="root"></div>
</body>
</html>

// index.js:
ReactDOM.render(
	<App/>,
	document.getElementById( 'root' ),
);
// App.js:
import React from 'react';

class App extends React.Component {
	render() {
		return (
			<div>
				Hello World
			</div>
		);
	}
}

export default App;
\end{lstlisting}

We define a class component by extending from \emph{React.Component}. Each class component must at least have a \emph{render()} method. React will use the return values of these methods to build the DOM.

When we start the react app, open it in the browser and select inspect we will see the following in the "Elements" tab (ignoring a script for live updates in the head):

\begin{lstlisting}
<html> 
<head>
	<title>Intro-App</title>
</head>
<body>
	<div id="root">
		<div>
			Hello World!
		</div>
	</div>
</body>
</html>
\end{lstlisting}

React starts traversing at whatever component is put into the \emph{ReactDOM.render} method and repeats the process for each component until primitive html elements that can be rendered directly are reached.

Every react class components can receive values from its parent element, by passing them like normal html properties (e.g. line 3 below). In the child they can be accessed through a variable called \emph{props} and using JSX we can embed the value of variables directly in the component (e.g. line 14 below):

\begin{lstlisting}[caption={Using Props},label={ex252}]
// index.js:
ReactDOM.render(
	<App text={ 'Hello World' },
	document.getElementById( 'root' ),
);

// App.js:
import React from 'react';

class App extends React.Component {
	render() {
		return (
			<div>
				{ this.props.text }
			</div>
		);
	}
}
export default App;
\end{lstlisting}
Both \textbf{\ref{ex251}} and \textbf{\ref{ex252}} will produce the exact same output.

\subsubsection{State}
React class components have a state which can be used to manage user actions on a component, as well as general application data. \emph{state} is a simple JavaScript object but should be treated as immutable and only be updated through the \emph{setState()} method. Modifying the state directly can lead to bugs and/or unexpected behavior of the application.

To demonstrate this, we'll add a \emph{Counter} component to the application:

\begin{lstlisting}
// index.js
import React from 'react';
import ReactDOM from 'react-dom';
import App from './App';

ReactDOM.render(
	<App text={ 'Hello World' }/>,
	document.getElementById( 'root' ),
);

// App.js
class App extends React.Component {
	render() {
		return (
			<div>
				{ this.props.text }
				<Counter/>
			</div>
		);
	}
}
export default App;

// Counter.js
import React from 'react';

class Counter extends React.Component {
	constructor( props ) {
		super( props );
		this.state = { val: 0 };
		this.increase = this.increase.bind( this );
		this.decrease = this.decrease.bind( this );
	}

	increase( e ) {
		e.stopPropagation();
		let { val } = this.state;
		val++;
		this.setState( { val } );
	}

	decrease( e ) {
		e.stopPropagation();
		let { val } = this.state;
		val--;
		this.setState( { val } );
	}

	render() {
		return (
			<div>
				<p>Current score: { this.state.val }</p>
				<button onClick={ this.increase }>+</button>
				<button onClick={ this.decrease }>-</button>
			</div>
		);
	}
}
export default Counter;
\end{lstlisting}

When defining a custom constructor for a class component it is necessary to call \emph{super( props )} first. To define an initial state of the component we can set \emph{this.state = \{ val: 0 \}}. This is the only place where state should modified directly. In comparison to that in lines 43 and 50 we first create a copy of the value we want to modify by using Object Destructuring and then use \emph{this.setState( \{ val \} )} to update it. By doing so we do not modify the state object directly.

In lines 31 and 32 we bind the this keyword on the increase and decrease functions. By leaving this step, accessing \emph{this.setState()} in any of the methods would crash the application as \emph{this} would be the global window object which doesn't have a \emph{setState} method defined.

The render method defines the output of the component. We'll render a paragraph telling the current score by accessing the state together with two buttons and their respective click handlers.

\subsubsection{Updating Components}
React is known for its good performance \cite{LinaSpukas} even in large applications. To understand better how it achieves this, we will look a bit at the internal process of rendering and updating the components.

On the initial render React will create a component tree from which it'll then build the DOM that the browser converts into displayable objects and paints them on the screen. To show how react determines which part of the DOM it has to update, we'll quickly walk through the previous example:

When clicking the increase or decrease button in the previous example we update the components state which will automatically trigger a re-render. In such a small component it wouldn't really matter if React simply rendered the whole component. In a component containing hundreds of lines and probably many other sub-components the decision to render all of just because, would take a long time and be fatal for performance especially if really all we'd have to do is re-render line 52.

\subsubsection{Finding The Differences} (maybe when coming back to this check https://reactjs.org/docs/faq-internals.html https://reactjs.org/docs/reconciliation.html and https://github.com/acdlite/react-fiber-architecture  and THIS:: https://www.youtube.com/watch?v=RquK3TImY9U )
To know which lines to update, React performs a few steps: 
\begin{itemize}
\item The \emph{setState} function will mark the component and all its children as dirty. \cite{GethylGeorgeKurian}
\item It'll recursively walk through all components marked as dirty, building them in the virtual DOM
\item For each built element it compares it to the value in the actual DOM. If they differ it'll deduce if the item should be replaced, removed or updated and does so accordingly. 
%https://overreacted.io/react-as-a-ui-runtime/#reconciliation
\end{itemize}
By doing so, instead of re-rendering the whole \emph{Counter} component it only re-renders
\begin{lstlisting}
<p>Current score: { this.state.val } </p>
\end{lstlisting}

\subsubsection{Stateless Functional Components}
It is also possible to create stateless functional components. In their pure form they do not contain state and normally only show either static data or data they get passed through props. If we look at the \emph{App} component in the above examples we'll find that it does exactly that. Knowing this, we could re-write it:
\begin{lstlisting}
// App.js
import React from 'react';
import Counter from './Counter';

function App( props ) {
	return (
		<div>
			{ props.text }
			<Counter/>
		</div>
	);
}
export default App;
\end{lstlisting}
Which is a lot shorter and has another big advantage: It protects from laziness. \cite{CoryHouse} As this component doesn't support local state it is not too tempting to quickly hack something new into it. Rather the programmer gets encouraged to think about the structure and create a proper component for a new feature together with its own state object that only that component needs.

And of course visually the result is equal to the one in \textbf{\ref{ex251}} and \textbf{\ref{ex252}}.

\section{Client - ApolloClient}
ApolloClient is a state management library for JavaScript that manages data with GraphQL. It offers an all in one solution for fetching, caching and modifying application data and together with automatic UI updates upon events from the server. \cite{ApolloClientIntro}

\subsubsection{Hooks}
ApolloClient 3 offers this by providing custom \textbf{hooks}. Hooks are a new addition to React since React 16.8 and were introduced to improve stateless functional components \cite{ReactHooksIntro} and can only be used in such. \cite{ReactHooksOverview} There were a few reasons that led to the introduction of hooks like the appearance of complex class components that couldn't be split into smaller ones, not re-usable stateful logic and classes being not ideal for future optimization. \cite{ReactHooksIntro} Hooks allow for example the usage of state in functional components.

\subsubsection{Apollo Hooks}
In addition to some built-in Hooks provided by React it is also possible to create them. Apollo implemented many own hooks, we will focus on \emph{useQuery}, \emph{useLazyQuery} and \emph{useMutation}.
The first argument to all of these is a GraphQL string. A query for the first two, a mutation for the last one.

The second argument is the options object. By adding properties to this the execution behavior can be influenced. Probably the most important argument is \emph{variables}. This is an object containing key-pair values that equal to the ones used in a GraphQL query as shown in \textbf{\ref{ex211}}. In addition to that there are \emph{onError} and \emph{onCompleted}. These are two callback functions that allow for executing actions upon completion or handling of possible errors. Imagine \emph{GET\_DATA} being a valid GraphQL query string:
\begin{lstlisting}
function Test() {
	useQuery( GET_DATA, {
		variables: { id: 1 },
		onCompleted: data => console.log( data );
		onError: error => console.log( error.message );
	} );
	...
}
\end{lstlisting}

For the rest of the arguments please refer to API reference form Apollo.

To be able to use returned data and inform the user about the current status, all of these three hooks return a few other objects, the most important being \emph{data}, \emph{loading} and \emph{error}. These are boolean values and allow for conditional rendering inside a component, depending on their values: 
\begin{lstlisting}
function Test() {
	const { data, loading, error } = useQuery( GET_DATA, {
		...	
	} );
	
	if ( loading ) return 'Fetching data.';
	if ( error ) return 'Error when fetching data.';
	return 'Success!';
}
\end{lstlisting}

In comparison to \emph{useQuery} the other two also return a function object that can be called when we wish to execute the query or mutation (see example below).

The \emph{useMutation} hook is the only one of the three allowing for an \emph{update} argument in the options object. In this we can access the local cache and have access to the results returned from the mutation. This can be useful if we need to update internal data depending on the result of a server operation:
\begin{lstlisting}[caption={Creating a Mutation},label={ex:usingMutation}]
function Mutate() {
	const [ runMutation, { data, loading, error } ] = useMutation( DO_THING, {
		update: ( cache, { data } ) => {
			// update cache with return data from mutation
		}	
	} );
}
\end{lstlisting}
This approach works well and is more or less the only way of handling return results from external mutations. 

\subsubsection{Local Resolvers}
We could do the same for local state changes, but as these functions can become long it might clutter the component with a lot of code that is not actually for it. To get rid of this problem we can define local resolvers in the client instance:
\begin{lstlisting}[caption={Local Resolvers},label={ex253}]
const client = new ApolloClient({
	...,
	resolvers: {
		Mutation: {
			setData: ( _, variables, { cache } ) => {
				// manage state update			
			},
			...		
		},	
	},
	...,
});
\end{lstlisting}

Just like on the server side we set up resolvers and can access query variables to it. To call it, we also define a GraphQL query like so:
\begin{lstlisting}
const SET_DATA = gql`
	mutation SetData($data: [String]!) {
		setData(data: $data) @client	
	}
`;
\end{lstlisting}
The name in line 3 has to match with the one defined in the resolver in \textbf{\ref{ex253}} in line 5. But whats even more important is the \emph{@client} directive in the end of line 3. This tells Apollo to not contact the server, but rather resolve the mutation locally. We can then call the above defined mutation like this:
\begin{lstlisting}
const Test() {
	const [ runSetData ] = useMutation( SET_DATA );
	
	const handleClick = e => {
		e.stopPropagation();
		runSetData(	{ variables: { x: 'test' } } );
	}
	
	return (
		<button onClick={ handleClick }>Click me</button>
	);
}
\end{lstlisting}

\subsubsection{The Apollo Cache}
A unique feature of Apollo is its smart cache. Once a query to the server has been executed, the results are saved in the cache and should the exact same query be executed again, the cache will first check if it has results for this query saved locally. If so, it'll return them from there, reducing network traffic and improving performance of the whole application. 

Of course in some cases it might be necessary to always have the newest data from the cache. The exact behavior can be specified through the \emph{options} object passed to a query or mutation.
\\

The setup time for working with Apollo Client is very short as it offers great default settings that will get any application going quickly. We just looked at the absolute basics of it, it offers features like pagination, subscriptions, optimistic UI and much more.
Further it allows the programmer to define all its behavior precisely when he wants to and by that its a great tool to work with.

\chapter{Deployment}

While developing in a local environment it is also important to test the application in a production environment as early as possible. Many frameworks make optimizations to improve performance. This might lead to errors that can't be seen in a local setup. Often they also change error behavior. While during development an error might only lead to a warning message, in production the same error might cause the application to crash.

Another reason to go for early deployment is to allow other people to see, use and test the application. Someone who sees the app for the first time or at least doesn't know how things were thought to work will make completely different use of it than a programmer. New issues with the workflow can be discovered and problems that a developer would never have thought of be reported. Further the layout will be tested on different screens, browsers and devices, unveiling unknown layout troubles. Issues discovered early are much easier to fix than those who are integrated deeply into the codebase. \\

In this chapter we will talk about the tools that were used to deploy the application to a publicly available domain.

\section{AWS}
AWS is a cloud platform from Amazon that offers over 175 different services in 24 different geographic regions around the world to allow for high availability all around the world. \cite{WhatIsAws}\\
AWS was launched in 2002 only offering SOAP (a messaging protocol used in distributed systems to exchange information) and XML interfaces. \cite{HistoryAwsMedia} In 2003 the idea of a big system offering services and tools was born as the leadership realized that one of the company's strengths was building and running effective, reliable and scalable data centers. After developing a more precise idea of how this strengths could be used to fulfill a lot of companies' needs the first public launch was in 2006 with \emph{Simple Storage Service} (S3), the \emph{Elastic Compute Cloud} (EC2) and a \emph{Simple Queue Service} (SQS) following shortly after. \cite{HistoryAwsTech}

Since then the services offered by AWS keep growing and other companies like Google, Microsoft or IBM started to compete in the business. In a 2019 research from Gartner AWS was found as one of the leading cloud Infrastructure as a Service providers world wide (IaaS) \cite{Gartner} 
regarding \emph{Completeness of Vision} and \emph{Ability to execute}. 

\subsection{AWS-EC2}
EC2 is a service that allows the creation of virtual machines with over 300 different variations of computational capacities like the number of CPUs, Memory, Storage and Network Performance. \cite{AwsEc2} 
Available configurations that can run on an instance are called Amazon Machine Images (AMIs) and include various Linux based distributions like Ubuntu, Debian or fedora as well as Windows. In addition to empty operating systems Amazon and different communities also provides images with certain pre-installed software making sure that instances can be set up quickly. \\
Basically, an EC2 instance is a virtual machine that runs on an Amazon server. \cite{Ec2VirtualMachine}

%In this app we will make use of EC2 to run a Neo4j instance. To make configuration easier, the Neo4j community provides images that can be started directly with only a few clicks from the AWS EC2 management console. By selecting AMIs and searching for "neo4j-community" in public images we can see all the available images with different versions of Neo4j. We will select one of the images that contains \emph{apoc} in their name to be able to use Cypher in our GraphQL schema. After clicking Launch we can select an instance https://neo4j.com/developer/neo4j-cloud-aws-ec2-ami/

\subsection{AWS-ECS}
RE-WRITE THIS
ECS is a service to run automatically managed containers on AWS servers. Many big companies make use of this service because of its scalability, reliability and security. \cite{AwsEc2}
These are the most important terms when talking about AWS ECS: \\

A \textbf{task definition} is the blueprint of a task, specifying which Docker image(s) to use, ports to expose, can set environment variables and memory needed etc. \cite{CodeCampEcs}

A \textbf{task} is an instance that is created following the specifications in the task definition. A task can run various Docker containers at once. A task is where an application actually gets executed.

A \textbf{service} is used to manage tasks. It is possible to specify a minimum and maximum number of running tasks and when to start new or stop running tasks to make sure the necessary calculation power is always necessary. A service can make use of a load balancer which will distribute network traffic equally over all running tasks in the service.

A \textbf{Cluster} is a logical grouping of services. \\

These components can be visually put together as shown in the figure below.
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{Bilder/EcsStructure.png}
\captionof{figure}{Strucute of AWS ECS}
\label{ex311}
\end{figure}

The orange square symbolizes ECS inside the big AWS infrastructure. We will ignore the square denoted with "VPC" as it is not relevant for further understanding.\\
A task is created from a task definition and be placed in a service. The services are here shown as light blue squares labeled \emph{Fargate}. Fargate is a computation engine from Amazon which calculates which virtual machine to use according to the task definition and creates and starts it and then runs the docker image from a specified container registry automatically. \cite{AwsFargate} The orange square with dots and dashes is a cluster. This cluster contains 2 services, reach running 3 tasks. 

The services of a cluster can be run in different \emph{Availability Zones} (AZ), meaning that they run on servers from AWS located in different parts of a geological region.

\subsection{AWS-Amplify}
Amplify is a services that aims to make the deployment as well as the development of applications as easy as possible. It comes with many advanced features that help setting up an app quickly. Some examples are: Authentication, API creation, Analytics, Push Notifications and many more \cite{AwsAmplify}. It creates a certificate for any deployed application, allowing for HTTPS connections. Further it is able to scan a connected GitHub repository and provide a template configuration based on the framework used.

Another really nice feature is the easy deployment flow: For each application it is possible to connect various branches. Each branch will have its unique URL and once a developer pushes to a connected branch, Amplify will build the newest status of the application. This is a great feature for testing new features in a production environment without having to deploy to a master server directly.

\section{Docker}
Docker is a platform that provides the ability to a developer to create any environment in which he wants to execute code in.
Before comparing it to a virtual machine we'll take a look at the basic terminology:

A \emph{Docker File} is a text file containing instructions that tell the Docker Engine how to build a \emph{Docker Image}. \cite{NickDocker}

This \emph{Docker Image} is a file containing necessary data to execute a given program. The in the docker file specified libraries are saved, folders from the local machine are copied, environment variables are set etc.. 

When running the image, we will get a \emph{Docker Container}. It will execute specified commands and by that for example download node modules. This container is an instance executed by the \emph{Docker Engine} which runs on top of the Host OS of an actual machine.
\\

While it is similar to a virtual machine, there are certain differences between them: \\
\textbf{Operating System} Every VM comes with its own operating system, making it heavy in terms of memory and processing power they require. Docker containers in turn all share the hosts operating system and only require the docker engine to be installed on the machine: \cite{GeekDocker}
\begin{figure}[H]
\centering
\includegraphics[scale=.8]{Bilder/DockerVsVM.png}
\captionof{figure}{Docker vs VM}
\label{ex312}
\end{figure}
\noindent
\textbf{Security} Following the previous point, every VM has its own operating system and is strongly isolated in the host kernel. Docker containers all run on a single kernel. Furthermore docker resources are shared. If an attacker gets access to one container he'll be able to exploit  all containers in a cluster. \cite{GeekDocker}) \\
\textbf{Portability} Containers can easily be ported to any machine that has the docker engine installed. There is no further configuration necessary, they'll the same on any machine. VMs are more difficult to port just due to their sheer size. In addition, the process of setting up a virtual machine differs from operating system to operating system. 

\chapter{Development}
In this chapter we will talk about the steps in development and explain some thought processes on the way. There might and probably will be parts with flaws in my work. This is intended to keep you from making the same mistakes when starting to develop an application with the GRANDstack. 

This chapter will also contain a few difficulties encountered together with a workaround found. Again, this part is not supposed to be a "How to" kind of guide - I was completely new to all of the technologies used when starting this project - but rather to document some decisions and the reasons why they were made.

\section{Why the GRANDstack}
To find out which tech stack to use, the first step was to analyze what the application should be able to do:
\begin{itemize}
\item The app will be interactive, not purely informational
\item The user will create, modify and delete data
\item Local changes made to data can be saved permanently
\item Data must be displayed as graph
\item At some point we might want to be able to implement real time updates for changes made by another user
\end{itemize}
This will give a list of requirements our tech stack should fulfill:
\begin{itemize}
\item Updating the DOM should be done efficiently as the layout will change depending on user actions
\item A local state management will be needed to keep track of local changes
\item A database will be necessary to permanently save changes
\item An intuitive way for saving graph data
\item Communication with the server should be lightweight and easy to implement
\end{itemize}

As we have looked at the individual parts of the GRANDstack, we can see quickly that it contains most of the things we imagined to be useful: Neo4j is a graph database, actually saves a graph on disk and is extremely performant. GraphQL will allows to only transmit the data needed in case of a lot of communication with the server. Together with Apollo, which offers a lot of help when implementing subscriptions, live updates should not be too hard to realize. Having a state management framework and a backend server that are designed to work together also promises for a trouble free development. The Apollo hooks make it very intuitive to update the UI according to state changes.

Furthermore this stack contains many relatively young technologies and as mentioned in chapter 1 this project also serves to explore these.

\section{The GraphQL Schema}
Development of applications that leverage GraphQL usually start by creating a schema to define how data looks like. The good thing about this is that it can be adapted if during the process we discover that we need to make some changes to it.

Here we will take a brief look at the GraphQL schema this app uses and I'll mention a few things from a personal point of view that didn't work out as I thought they would.
\begin{lstlisting}[caption={GraphQL Enums},label={enums}]
enum NodeType{
  API
  Command
  Query
  Event
  Persistence
  AbstractUserInterface
  Object
  Computation
  Container
  Domain
  Invariant
  ArchitecturalDecisionRecord
  Definition
}

enum LinkType{
  PartOf
  Trigger
  Read
  Mutate
  Generic
}
\end{lstlisting}
The value of \emph{NodeType} or \emph{LinkType} when creating or updating a node or link sent to the server must be one of these. Any other value will result in an error.

\begin{lstlisting}
enum ArrowType{
  Default
  none
  SharpArrow
  Curve
  Diamond
  Arrow
  Box
  Triangle
  Bar
  InvTriangle
}
\end{lstlisting}
These are the values an arrow on a link displayed in the application can have. The actual values in this enum depend highly on the options from the canvas library used (in this example visjs).

\begin{lstlisting}
interface IDisplayable{
  """
  Minimal data necessary for the object to appear on screen
  """
  id: ID!
  label: String
  story: URI
}

type Node implements INode & IDisplayable{
  id: ID!
  label: String!
  nodeType: NodeType!
  story: URI
  Links: [Link] @cypher(statement: "MATCH (this)--(l:Link) RETURN l")
  synchronous: Boolean
  unreliable: Boolean
  connectedTo: [Node] @cypher(statement: "MATCH (this)--(:Link)--(n:Node) return n")
}
  
type Link implements ILink & IDisplayable{
  id: ID!
  label: String!
  linkType: LinkType!
  x: Node! @cypher(statement: "MATCH (this)-[:X_NODE]->(n:Node) RETURN n")
  y: Node! @cypher(statement: "MATCH (this)-[:Y_NODE]->(n:Node) RETURN n")
  x_end: LinkEnd @cypher(statement: "MATCH (this)-[:X_END]->(le:LinkEnd) RETURN le")
  y_end: LinkEnd @cypher(statement: "MATCH (this)-[:Y_END]->(le:LinkEnd) RETURN le")
  sequence: SequenceProperty @cypher(statement: "MATCH (this)-[:IS]->(s:Sequence) RETURN s")
  story: URI
  optional: Boolean
}

\end{lstlisting}
Coming from an Object Oriented background I wanted to use interfaces.  Having an ID, label and story object to be apparent on all entities that can be shown on screen makes sense. What I didn't know when creating the schema is that visjs requires each link to be connected to an x and y node, in other words: a link can't "float". This means that there are two more required fields in the ILink interface, making the IDisplayable interface more or less loosing its purpose of defining properties that are required to be displayed.

To retrieve the links attached to the node we make use of the \emph{@cypher} directive we talked about earlier. This is an array which can be empty as nodes do not necessarily need be connected in any way. Same counts for the array of connected nodes. The two booleans \emph{synchronous} and \emph{unreliable} were subject to a lot of discussion as they could also be placed on links rather that nodes. In the end I decided to place them on nodes as it made more sense to me that for example an event would be synchronous rather than the dispatch call for it.

\emph{optional} on link is a value that can be used to clarify that a link of a sequence will only be done in certain cases.

\begin{lstlisting}
type SequenceProperty{
  group: String
  seq: Int
}

input NodeInput{
  label: String
  story: URI
  nodeType: NodeType
  synchronous: Boolean
  unreliable: Boolean
}

input NodeCreateInput{
  synchronous: Boolean
  unreliable: Boolean
  story: URI
}

type LinkEnd{
  note: String
  arrow: ArrowType
}
\end{lstlisting}
Every sequence on a link can contain a string identifying the sequence group it belongs to as well as a sequence number to display the order in which steps will be executed. Each link end can contain a note about how the component on the respective end perceives the incoming connection.

The input types are used to be able to pass an object into GraphQL queries or mutations instead of just primitive datatypes. This idea sounds promising as we can just pass all input objects from a form to the query. But as we can see in the above code snippet there is one input type for creating and one for updating nodes. The reason for that is that we'd pass all optional parameters as an input object and the required ones manually. In the end it turned out that this led to a lot of object modifying just to get the right structure from the input types.

This schema could be improved by using GraphQL fragments and make it shorter by that, but the usage of these input types was not worth the effort. 

\begin{lstlisting}[caption={Mutation Type Definition},label={mutations}]
\label{exMutations}
type Mutation{
  SeedDB: seedReturn
  CreateNode(id: ID!, label: String!, nodeType: NodeType!, props: NodeCreateInput): NodeOperationReturn
  CreateLink(id: ID!, label: String!, x_id: ID!, y_id: ID!, linkType: LinkType!, props: LinkCreateInput): LinkOperationReturn
  CreateSequence(link_id: ID!, props: SequencePropertyInput): SequenceOperationReturn
  CreateLinkEnd(link_id: ID!, props: LinkEndInput): LinkEndOperationReturn

  MergeSequence(link_id: ID!, props: SequencePropertyInput): SequenceOperationReturn
  MergeLinkEnd(link_id: ID!, props: LinkEndInput): LinkEndOperationReturn

  UpdateNode(id: ID!, props: NodeInput): NodeOperationReturn
  UpdateLink(id: ID!, props: LinkInput): LinkOperationReturn
  UpdateSequence(link_id: ID!, props: SequencePropertyInput): SequenceOperationReturn
  UpdateLinkEnd(link_id: ID!, props: LinkEndInput): LinkEndOperationReturn

  DeleteNode(id: ID!): deleteReturn
  DeleteLink(id: ID!): deleteReturn
  DeleteSequence(link_id: ID!): deleteReturn
  DeleteLinkEnd(link_id: ID!, xy: String!): deleteReturn

  RequestEditRights: EditRightOperationReturn
  FreeEditRights: EditRightOperationReturn
}
\end{lstlisting}

This is the definition of the root mutation type. It clarifies even more the previously made point of having a lot of trouble formatting the inputs for a mutation: When dispatching a \emph{CreateNode} mutation 2 (label and nodeType) of the form inputs have to be passed directly, while all others need to be put into an object called props that contains the fields defined in the respective input type. This makes the code for form handling a lot harder to re-use, especially as the input types vary for updating and creating.

Lets talk about the return types defined for each operation:
\begin{lstlisting}
interface IReturnInfo{
  success: Boolean!
  message: String
}
type NodeOperationReturn implements IReturnInfo {
  success: Boolean!
  message: String
  node: Node
}
type LinkOperationReturn implements IReturnInfo{
  success: Boolean!
  message: String
  link: Link
}
\end{lstlisting}
Similar to how a project using a REST-API would implement it, this was the idea of the process:
\begin{itemize}
\item Frontend dispatches a call to the server
\item Server starts resolving
\item Resolving is successful
	\begin{itemize}
		\item Server sets \emph{success} to true and adds the created object to the payload
	\end{itemize}
\item Resolving fails
	\begin{itemize}
		\item Server sets \emph{success} to false and adds an error message
	\end{itemize}

\item Frontend checks \emph{success} and depending on its value will either display data about the modified object(s) or the error message defined by the server
\end{itemize}

Maybe it was due to missing experience, but it didn't seem intuitive at all to define custom responses from the ApolloServer to the client. Just by accident the manner to return custom error messages appeared, but it was already late in development process and there was not enough time left to build it into all places where it would've been necessary. More about this in \autoref{chap:LB}.

\begin{lstlisting}
type Query{
  Nodes: [Node]
  Links: [Link]
  IsProjectBeingEdited: EditRightQueryReturn
}

schema {
  mutation: Mutation
  query: Query
}
\end{lstlisting}

The end of the schema contains the definition for the query type and the root schema.

\section{Getting started with Neo4j}
To get started with Neo4j we first want to create a small basic data set that shouldn't be too complicated so we can easily experiment with it. So lets try to create a representation of the following graph in the database:

\begin{figure}[H]
\centering
\includegraphics[scale=.8]{Bilder/BasicGraph.png}
\captionof{figure}{A small network of software components}
\label{ex421}
\end{figure}

This is the network we would like to display in the application. It is really helpful to first draw it on paper and then think of the individual components needed. As there will be one graph in the database and one in the application it is important to carefully separate them when talking about a node: This could refer to one either in the database or in the application. While the node in the app is always a node in the database, this is not necessarily the case the other way around as a node in the database could also represent data about a connection in the application.

We will create the three nodes for DB, Backend and Frontend using the following code in the Neo4j Browser:

\begin{lstlisting}
CREATE (:Node:AbstractUserInterface {id: randomUUID(), label: "Frontend", story: "Interaction point for the user", nodeType: "AbstractUserInterface"})
CREATE (:Node:API {id: randomUUID(), label: "Backend", story: "Endpoint for requests, fetches from and mutates data on the DB", nodeType: "API"})
CREATE (:Node:Persistence {id: randomUUID(), label: "DB", story: "Saves data for the methodical designer", nodeType: "Persistence"})
\end{lstlisting}

Everything between the parentheses of a create statement defines a new node in Neo4j. By using colons we define labels for a node. These can be used to filter for a certain type of node and can improve performance when traversing. 

All of them are of type node and each of them has its own specific label, marking them as different node types. By using curly braces we define properties on each of them: A unique ID for each node by using Neo4j's \emph{randomUUID()} function, a label which will be displayed in the application and a story which shortly describes its functionality for each node. Furthermore we give each node a nodeType. This might seem a bit counter intuitive as this was already defined in the labels for the node. However later when retrieving the nodes from the database it'd only be possible to retrieve \emph{all} labels of a node by using Neo4j's \emph{RETURN labels(n)} function. This would return a list, but for displaying them we only need one which is why we will include this information in the node's properties as well.

This is how the created graph looks like in the Neo4j Browser:
\begin{figure}[H]
\centering
\includegraphics[scale=1]{Bilder/BasicGraphNeo1.png}
\captionof{figure}{The nodes in Neo4j Browser}
\label{ex422}
\end{figure}

Before directly creating the respective connections between the nodes to complete the data for the image lets first think about what properties such a link might contain later:
\begin{itemize}
\item Data about the x-end of the connection (arrow-type, color, an annotation about how the x-node component sees the connection)
\item The same for the y-end
\item Information about if the link is part of a sequence of steps and if so a short description of the step
\item A label, id, story and linkType
\item The IDs of the nodes it connects
\end{itemize}

We could save all this information on the relationship between the nodes. But if the user now updates only the label of a node and saves this change, the frontend will send all fields and the DB will have to write all other properties as well without actually modifying them.

To somewhat minimize the amount of data sent it might be a good idea to split the information among various nodes:
\begin{itemize}
\item One link-node that containing the label, id, story and linkType
\item Two link-end-nodes containing visual information about the arrow as well as a "note" field to clarify how one node might perceive the connection
\item One sequence-node to specify whether or not the node is part of a sequence
\end{itemize}

The link-node would have direct connections to the components it connects. The sequence- and link-end-nodes would be connected to their link-node and can be accessed by finding the link and from there looking for respective connections.

Having this in mind lets go on to create the link-nodes and attach them to their components. For now lets not create any sequence- or link-end-nodes to keep things simple:

\begin{lstlisting}
MATCH (api:API) WHERE api.label = "Backend"
MATCH (db:Persistence) WHERE db.label = "DB"
CREATE (l:Link)
SET l.label = "Mutates", l.linkType = "Mutate", l.story = "See JIRA for details: https://.."
CREATE (api)<-[:IS_X]-(l)-[:IS_Y]->(db)
\end{lstlisting}

This code will find a node with an \emph{API} label and one with a \emph{Persistence} label. We can be sure that this will find the correct nodes in this example because only one of each type exists. Normally their node IDs would be necessary to identify them. Putting a string in front of the first colon in the match case will declare a variable name for the found node which can be used later to reference it. Then the code will create a link from api to DB, giving it a \emph{Link} label and assigning the link the variable name \emph{l}. Using this variable we set the link's properties label, linkType and story. This time the story serves as reference to an external document describing the relationship in more detail. Then using another create statement Neo4j will create a connection between the link-node (a Neo4j node representing a link in the application) and the node-node (a Neo4j node representing a node in the application). After executing the Neo4j Browser will show the following image:
\begin{figure}[H]
\centering
\includegraphics[scale=1]{Bilder/BasicGraphNeo2.png}
\captionof{figure}{After creating the first link-node}
\label{ex423}
\end{figure}

After adding the rest of the connection using the same approach the whole representation of \textbf{figure \ref{ex421}} on the data layer looks like:
\begin{figure}[H]
\centering
\includegraphics[scale=1]{Bilder/BasicGraphNeo3.png}
\captionof{figure}{Whole graph in the DB}
\label{ex423}
\end{figure}
It is important to note that directions of the arrows here do not represent the direction of the link in the image. They are only necessary because Neo4j doesn't allow for undirected links in create statements. To make it uniform we made all relationships to be outgoing from the link nodes.

\section{Communicating with the DB through ApolloServer and GraphQL-Playground}
The next step is to communicate with the database through the server. After we set up a small ApolloServer (more on setup in \autoref{chap:Doc}) we should create a resolver to seed the database with some default data similar to the one shown previously in \autoref{ex423}. This was really helpful as especially in the beginning we might make many mistakes which can leave the dataset in a bad shape and fixing it manually would take a lot of time. In \autoref{mutations} in line 2 is the GraphQL definition. The resolver looks like the following:
\begin{lstlisting}
const seedQuery = require( './seed' );
async SeedDB( _, __, ctx ) {
	try {
		const session = ctx.driver.session();
		const deleteQuery = `
			MATCH (n) DETACH DELETE n
		`;
		await session.run( deleteQuery );
		await session.run( seedQuery );
		await session.close();
		return {
			success: true,
		};
	}
	catch( e ) {
		return {
			success: false,
		};	
	}
},
\end{lstlisting}

The \emph{seedQuery} is a long cypher query defined in an external file. After deleting all nodes and connections in the database using the \emph{deleteQuery} we run it to create new data to use. The reason why we can return success is that there is nowhere where the query could fail.

To run this resolver and seed our database we will open the GraphQL Playground in the browser and execute the following GraphQL query:
\begin{lstlisting}[caption={Seeding the DB through GraphQL Playground}, label={exSeed}]
mutation seedDB {
  SeedDB {
    success
  }
}
\end{lstlisting}
The name in line 1 is completely optional. What matters is line 2 as this string will be used to identify the correct resolver. Using the curly braces we define a result set. After hitting the play button we will get the expected result:
\begin{lstlisting}[caption={Seeding Result}, label={exResSeed}]
{
  "data": {
    "SeedDB": {
      "success": true
    }
  }
}
\end{lstlisting}

The next step is retrieving data. To save some time when writing queries we will use the neo4j-graphql-js npm package which combines our resolvers and GraphQL schema to an executable schema. This package can generate many queries and mutations based on the schema we provide but still allows the usage of resolvers we define on our own.
By doing so, we can fetch nodes without any further coding:

\begin{lstlisting}[caption={Fetching Nodes}, label={exFetch}]
query nodes {
  Nodes {
    id
    nodeType
    label
  }
}
\end{lstlisting}

With the result being similar to:
\begin{lstlisting}[caption={Result Set}, label={exResFetch}]
{
  "data": {
    "Nodes": [
      {
        "id": "738e414d-bc1f-4e90-ad76-ec44d34f1a71",
        "nodeType": "AbstractUserInterface",
        "label": "UI"
      },
      {
        "id": "6c4b4dba-5726-47bc-8fb5-10affcf03ef7",
        "nodeType": "API",
        "label": "Server"
      },
      {
        "id": "2554b296-ffed-4028-ad80-1181dfe97ecd",
        "nodeType": "Persistence",
        "label": "NeoDB"
      },
      {
        "id": "ded515d5-8016-4324-a756-201b9e1f2db0",
        "nodeType": "Event",
        "label": "Create Node"
      }
    ]
  }
}
\end{lstlisting}

Finally lets write a resolver to create a node. The reason for doing this by hand is that we want to have control over the Cypher query, to be sure that the properties would be assigned the way we want it:
\begin{lstlisting}
		async CreateNode( _, args, ctx ) {
			try {
				const session = ctx.driver.session();
				const query = `
					CREATE (n:Node:${ args.nodeType } {id: $id, label: $label, nodeType: $nodeType})
					SET n += $props
					RETURN n`;
				const results = await session.run( query, args );
				await session.close();
				return {
					...defaultRes,
					node: PrepareReturn( results, 'n', defaultNode ),
				};
			}
			catch ( e ) {
				return errorRes( e );
			}
		}
\end{lstlisting}
In line 5 we make use of ES6 template strings and use the \emph{args} parameter to create the correct label in the query string, as it is not possible to use query variables in labels in Cypher. This example also shows the usage of query variables in Cypher really well. The \emph{args} object will be destructured and its keys are available to the query by using the \emph{\$} sign. Furthermore it demonstrates how to use input types in line 6 to set various properties at once using Cypher. In the GraphQL Playground it can be used like the following:
\begin{lstlisting}[caption={Using the Create Node resolver},label={ex:RunCreateNode}]
mutation createNode($props: NodeCreateInput) {
  CreateNode(id: "1", label: "new test", nodeType: Object, props: $props){
    success
    node {
      id
      label
    }
  }
}
\end{lstlisting}

And the \emph{Query Variables} section contains the contents for \emph{props}:
\begin{lstlisting}
{
  "props": {"synchronous": false, "unreliable": false, "story": "test"}
}
\end{lstlisting}

\section{Making ApolloServer and ApolloClient communicate}
After looking at how to run queries from the GraphQL-Playground and how they are resolved in the server, we not need to put those two parts together. The ApolloClient needs nothing more but a URI that points to the ApolloServer.

To then run the createNode mutation seen in \autoref{ex:RunCreateNode} we will make use of the \emph{useMutation} hook from Apollo shown in \autoref{ex:usingMutation}. To do so we first define the query object:

\begin{lstlisting}[label={ex:DefQuery}]
export const CREATE_NODE = gql`
  mutation($id: ID!, $label: String!, $nodeType: NodeType!, $props: NodeCreateInput){
    CreateNode(id: $id, label: $label, nodeType: $nodeType, props: $props) {
      success
      node {
        id
        label
        nodeType
        story
        synchronous
        unreliable
      }
    }
  }
`;
\end{lstlisting}

To use it we'll create a small functional component and pass the query to the hook:

\begin{lstlisting}[caption={Using the Query in a Component}]
import { CREATE_NODE } from './queries'

const NodeCreationComponent = () => {
	const [ runCreateNode ] = useMutation( CREATE_NODE );
	
	const handleSubmit = ( e ) => {
		e.stopPropagation();
		const { label, story, synchronous, nodeType, unreliable } = state; 
		const id = generateID();
		const variables = { id, label, nodeType, props: { story, synchronous, unreliable } }; 
		runCreateNode( { variables } );
	}
	
	const inputForm = () => { ... };
	
	return (
		<div>
			{ inputForm }
			<button onClick={ handleSubmit }>Save</button>
		</div>	
	);
}
\end{lstlisting}
In line 8 we retrieve the input fields from the form. To shorten the example the form is represented as JSX. This allows to save HTML or other react components in variables and render them by using curly braces.

Line 10 contains the creation of the variables object we pass to the mutation in line 11. It will destructure the object and pass on all properties.

Line 14 contains the JSX definition of the form and we use it in line 18 by using curly braces.

\section{Building the UI}
When building a React application its useful to think about the structure to get an idea of what components will be necessary. Furthermore if the application should be usable on mobile devices it might be a good idea to first create the layout for those. Moving then to a layout that is suitable for a PC screen is a lot easier than the other way around.

\subsection{Components}
Lets think about what will appear on the screen:

The application should have a big canvas to display the graph and allow interaction with it. There needs to be space to show information about a clicked link or node, as well as options to search/filter the view. A button to save data to the database and also to discard local changes should be apparent. So lets take a look at the app and at the components it consists of:
\begin{figure}[H]
\includegraphics[scale=.46]{Bilder/Layout.png}
\captionof{figure}{The Components in the UI}
\end{figure}
The top level components in red are:
\begin{itemize}
\item[1] \textbf{HeaderArea:} It is almost completely static and contains the name of the application, together with two other components that manage the communication with the server.
\item[2] \textbf{InteractionPane} All components that are required to modify the data of the application will go in here.
\item[3] \textbf{EditorPane} The canvas that contains the graph the user sees.
\end{itemize}

Their child components in green:
\begin{itemize}
\item[1] \textbf{OptionBar} It contains the buttons to create a node or a link, together with a button that will hide the entire \emph{InteractionPane} to give the \emph{EditorPane} the full window width.
\item[2] \textbf{InputPane} All components that require input from the user are child components of this one. This could be called a container component as it doesn't display anything on its own.
\item[3] \textbf{ProjectStatus} Shows information about if the user can make changes to the data and allows the request of editing rights to do so.
\item[4] \textbf{SavePane} Will send changed entities to the server and inform the user about the progress or discard local changes.
\end{itemize}

The child component of the \emph{InputPane} in cyan is called \textbf{GraphSettingsPane}. In the future there will be more options on manipulating data shown in the graph, like filtering for certain types, special layout algorithms etc. What component is displayed here varies depending on what the user last clicked. There are 2 more forms for creating and editing a node or a link.

\section{Problems}
This section will talk about difficulties or problems that appeared during development. For some of them there was a solution found, others are simply to document them.

\subsection{Keeping the data consistent when saving changes}
% In which order do the updates need to be executed?
In the very beginning the process of saving local changes to the database was the following: Go through all created/edited/deleted entities and for each of them dispatch a call to the server. This often led to inconsistent data sets that could not be displayed by visjs. To explain this problem we need to look at a few examples of saving scenarios:
\begin{itemize}
\item[1] The user creates 2 nodes and connects them with a link.

Despite this being a very simple example, the approach to save data in Neo4j can cause problems. When creating the link it will look for the IDs of both nodes to connect to. But if for example one of the calls to the database to create the nodes hasn't arrived or wasn't processed yet? Neo4j will throw an error because we're trying to work with a node that doesn't exist.

\item[2] The user deletes a node that has a circular link (both ends of the link point to the same node) attached to it.

Deleting such a node will also automatically lead to the deletion of the link. When the call to delete the node gets processed first, Neo4j will also throw an error as it doesn't allow to delete nodes with relationships.

\item[3] The user deletes a link that has a sequence and link-end property.

As discussed previously, these two exist as their own nodes in Neo4j. Simply deleting the link won't work as it has connected relationships. Using \emph{DETACH DELETE} to delete the node will at least not throw an error, but leave the sequence- and link-end-node fly around in the database with not chance to ever get deleted again except someone looks through the database and does so by hand.
\end{itemize}

There are many more cases where the execution order of queries is critical for keeping the database clean and there might be multiple ways of doing so. At the moment the application makes use of the ES6 function \emph{Promise.all()}. There is an array for each type of operation (creating node, creating link, deleting node, etc.). When the application finds an entity that needs to be sent to the server it will push the return value from the Apollo mutation hook into the respective array. Using \emph{Promise.all( iterable )} it waits for all promises of that type to resolve and only if that is the case it will continue the saving process.

The order used at the moment is the following:
\begin{itemize}
\item[1.] \textbf{Creating and updating nodes} First make sure that all nodes that might be needed later are created. In addition to that any updates made to other nodes can be done simultaneously as that won't affect other entities.
\item[2.] \textbf{Create Links} Add any new links to the database as they might be referenced in sequence- and link-end-creations in the next step.
\item[3.] \textbf{Sequences and Link-Ends} must be created on newly created links
\item[4.] \textbf{Update Links} Changes to Links can be commited safely to the database as any new node they might connect to is guaranteed to exist and internal updates won't affect other entities. In the same step we can update their link-end- and sequence-properties.
%CHECK IF THIS IS TRUE
\item[5.] \textbf{Delete Links} Remove deleted links from the database. This must be done before deleting nodes as otherwise deleting a node might not work if it has attached relationships.
\item[6.] \textbf{Delete Nodes} Remove deleted nodes from the database. Its important to do this using \emph{DELETE} and not \emph{DETACH DELETE} to make sure there won't be any links without node on one end.
\item[7.] \textbf{Reset Local Store} After that mark all items in the local store as up do date.
\end{itemize}

Here is a short piece of code to make the idea of promises better understandable:
\begin{lstlisting}[caption={Usage of Promise.all()}]
for ( let node of createdNodes ) {
	const { id, label, story, synchronous, nodeType, unreliable } = node;
	const variables = { id, label, nodeType, props: { story, synchronous, unreliable } };
	nodePromises.push( runCreateNode( { variables } ) );
}
for ( let node of editedNodes ) {
	const { id, label, story, synchronous, nodeType, unreliable } = node;
	const variables = { id, props: { label, nodeType, story, synchronous, unreliable } };
	nodePromises.push( runUpdateNode( { variables } ) );
}

Promise.all( nodePromises ) 
	.then( () => {
				for ( let link of createdLinks ) {
					...
					createLinkPromises.push( runCreateLink( { variables } ) );				
				}
				Promise.all( createLinkPromises )
					.then( () => {	
		...
\end{lstlisting}
In line 4 and 9 the promises are pushed into the array. All of the mutations are executed instantly. The \emph{then} branch of \emph{Promise.all()} in line 13 will only be reached if all of the promises in \emph{nodePromises} resolve. If that is the case, the mutations to create links can be run.

\subsection{AWS-Healthcheck}


\subsection{Apollo Error-Codes}
\subsection{Apollo Chrome Dev-Tools}
I often said that Apollo is a great tool and offers a lot of functionality. Debugging however was not such a pleasant experience as the Dev Tools seemed to have only around a 50\% chance of being able to connect to the application:
\begin{figure}[H]
\centering
\includegraphics[scale=.65]{Bilder/ScreenOfDeath.png}
\captionof{figure}{The Apollo Dev Tools can't connect}
\end{figure}
This became especially annoying when wanting to check the local state after completing certain steps and then having to re-load the page a few times.

\subsection{CORS-problems}
\section{Graph-Layout}
\subsubsection{Tree-Layout}
\subsubsection{Flower-Layout}
\section{Behavior Decisions}
\section{Avoiding Data Corruption}
\section{Detect Multiple Connections}

\chapter{Looking back}
\label{chap:LB}
Here we will first look at a list of things that were positive in this project and then go on to a others that maybe weren't ideal together with suggestions to some points on how they could be improved.

\section{What was good}
\begin{itemize}
\item[It works!] The application works and runs stable on a public domain without any known bugs in the functionality which is a big success.
\item[The tech-stack] In a long term view choosing the GRANDstack was a good idea. When the functionality for real time updates is getting implemented GraphQL will be even more useful and Apollo will help a lot doing so. In addition all components work together well and will allow for a comfortable further development and scaling.
\item[Interesting] Learning many new things from scratch at the beginning of the project is of course a lot to take in. But with time came more understanding of every part and it is interesting to take a deep dive into this area of software development and get to explore profoundly all parts of it.
\end{itemize}

\section{What was not ideal}
\begin{itemize}
\item[Warmup time] Trying to get going as quickly as possible right in the beginning was not best choice. The reason for doing so was to have something to show to others early in the development and to avoid having time issues later on. However, it might have turned out to be quicker to first get a good overview of all the technologies and frameworks, especially Apollo. It offers a lot of functionality and taking more time in the beginning could have been faster and led to a cleaner code base in the end.

\item[The tech-stack] Despite having it on the positive list it appears here as well, the reason being that in the short term the chosen technologies were over-kill in my opinion. Especially Apollo is a very big dependency, but the app only makes use of very small and specific parts of it, the most important one being state management. While it is not bad at it there is no big advantage of it over using a pure state management library like Redux. Furthermore having to define a GraphQL query for fetching the local state creates many lines of code and the queries all ended up being almost similar which invalidates the argument of getting only the needed data. In addition to that it made development slower because many times it comes up later that another property, or even a few, are needed which have to be added to the query. In the end queries often turn out so similar that the only logical thing would be to use the same query for many different purposes.

In addition looking at how the app works at the moment it could easily be implemented using a REST-API. On the initial start the app fetches all nodes and links from the database. When the user hits save it will send modified links and nodes to the server where their ID is used to modify them in the database. As nodes and links are the only two types of entities in this application (and it'll most likely stay that way) using a REST-API might have been the better choice.
\end{itemize}

\chapter{Documentation}
\label{chap:Doc}
\section{Running the App locally}

\chapter{Ideas for the Future}
